\section{Linear Equations}

  A linear equation over with unknowns is represented by $\forall \{a_1, \ldots, a_n\}, b \in K,\ \ a_1 x_1 + a_2 x_2 + \cdots + a_n x_n = b$.
  A system of linear equations is a set of $L$'s, where $L$ is a linear equation as above.
  Linear equations form a vector space that can be scaled.
  
  Given a system of linear equations with unknowns, a solution vector $u = (a_1,\ldots, a_n)$ is something that simultaneously satisfies all of the linear equations.
  
  \subsection{Definitions}
  
    \begin{description}
      \item[Augmented Matrix]  Given a system of linear equations defined above, the augmented matrix is:\\
      $M = \left[\begin{array}{cc|c}
        a_{11} & \ldots & b_1 \\
        \vdots & \ddots & \vdots \\
        a_{m1} & \ldots & b_m 
      \end{array}\right]$

      \item[Equivalent Systems of Linear Equations] Two system of linear equations are equivalent if they have the same system.
      More precisely, $M_1$ and $M_2$ are equivalent when the rows of one are linear combinations of the rows of the other. 
      This means that systems of linear equations are equivalen when they are row equivalent.

      \item[Elementary Row Operation (ERO)] An ERO is an operation of either interchanging two rows, replacing a row with a multiple of itself, or replacing a row with the addition of itself and another row.

      \item[Row Echelon Form] Row echelon form consists of a matrix such that each leftmost nonzero entry of a row is to the right of the leftmost nonzero entry of each proceeding row, all zero rows are at the bottom of the matrix.

      \item[Row Canonical Form (Reduced Row Echelon) (Fully Reduced)] Row canonical form or full reduced matrix form is a matrix that satisfies row echelon form where every first entry in each row is 1.

      \item[Pivot] A pivot is the first nonzero entry in a row of a matrix in row ecehlon form.

      \item[Gaussian Elimination] A matrix can be converted into reduced row echelon form by performing row separations to put it into echelon form, dividing throughout so that every pivot is 1, and then using the 1 to zero out every space above pivots. 

    \end{description}
  
  \subsection{Theorems}
  
    \subsubsection{Solution to Linear Equation}
    
      If $u$ is the solution to a linear equation where $L_1: x_1 + x_2 = 0; L_2 = x_1 - x_2 = 3; L_1 + L_2: 2x_1 = 3$ then $x_1 = 3/2, x_2 = -3/2$ by back substitution.
      
    \subsubsection{Non-Invertible Matrix}

      A matrix is not invertible if there exists some nonzero vector such that the matrix multiplied by the vector is 0.

    \subsubsection{Products of Invertible Matrices}

      A product of matrices is invertible if and only if each of the individual matrices are invertible.

    \subsubsection{A matrix is row equivalent to a unique matrix in RCF}
      
      Wellp... the subsection name sums it up.
      A proof is further below.

  \subsection{Proofs}

    \subsubsection{Every ERO has an inverse operation}

      BLURRY PAGE, GO BACK!

    \subsubsection{A product of a set of matrices is invertible if and only if each of the set of matrices is invertible}

      \begin{align}
        \forall invertible[A], \exists v \neq 0\ \ s.t.\ \ Av = 0 \\
        B = A^{-1} \\
        B(Av) = B \cdot [0] = [0] \\
        (BA)v = (A^{-1}A)v = Iv = v \implies v = 0
      \end{align}

      In words, since this proof can be somewhat confusing (or at least it confused me): if A is invertible there must be some nonzero vector such that Av is 0.
      Creating some matrix B that is the inverse of the vector A, should you multiply Av (0) by B it should be 0... or it could be v, depending on how you frame the problem.
      So v must be 0 and thus the matrix must be invertible.

      Another proof.
      Suppose a set of matrices A are all invertible and their product is B.
      If you repeatedly multiply both sides of this equality by a corresponding inverse of A to replace the set of matrices with the identity matrix, you will get $I = (A_n^{-1} \ldots A_1^{-1})B$.
      This can be reduced to $(A_n^{-1} \ldots A_1^{-1}) = B^{-1}$ by multiplying both sides by the inverse of B.
      Therefore, the inverse of B exists and the product of a set of invertible matrices is invertible.

      We can use this to show each element is invertible.
      If $A_1A_2\ldots A_n$ is invertible so is $(A_1\ldots A_{n-1})A_n$ by associativity.
      The expression has two terms and the term in paranetheses is invertible and $A_n$ is invertible if we know the second term.

    \subsubsection{A matrix is row equivalent to a unique matrix in RCF}

      We know that $M_1$ and $M_2$ represent equivalent linear systems if and only if these matrices are row equivalent, thus there is a sequence of EROs that can convert matrices to each other.
      $M_2$ can be transformed into a matrix $N$ in RCF by some sequence of EROS.
      $M_1$ can be transformed into a matrix $N$ in RCF by some sequence of EROS.
      All three of these matrices are equivalent.